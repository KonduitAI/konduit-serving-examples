{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ONNX Runtime with Konduit Serving to serve a PyTorch model\n",
    "\n",
    "The Open Neural Network Exchange (ONNX) format is supported by a number of deep learning frameworks, including PyTorch, CNTK and MXNet. \n",
    "\n",
    "This notebook provides an example of serving a model built in PyTorch with [ONNX Runtime](https://github.com/microsoft/onnxruntime), a cross-platform, high performance scoring engine for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements \n",
    "\n",
    "This tutorial requires a Python environment with PyTorch available. See the [PyTorch homepage](https://pytorch.org/) for instructions. \n",
    "\n",
    "In addition, you would need to have the following packages installed (package versions in brackets): \n",
    "- Pillow \n",
    "- ONNX \n",
    "- ONNX Runtime \n",
    "- OpenCV \n",
    "\n",
    "All of these packages **except ONNX Runtime** can be installed from the `conda-forge` channel: \n",
    "\n",
    "```bash \n",
    "conda install -c conda-forge pillow onnx opencv\n",
    "```\n",
    "\n",
    "Install ONNX Runtime from pip: \n",
    "```bash\n",
    "pip install onnxruntime\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from urllib.request import urlretrieve \n",
    "import sys \n",
    "import numpy as np \n",
    "import time \n",
    "from PIL import Image \n",
    "\n",
    "import onnx\n",
    "from onnx import optimizer\n",
    "\n",
    "from konduit import PythonConfig, ServingConfig, InferenceConfiguration, PythonStep\n",
    "from konduit.server import Server\n",
    "from konduit.client import Client \n",
    "from konduit.utils import default_python_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download file \n",
    "\n",
    "For the purposes of this example, we use ONNX model files from [Ultra-Light-Fast-Generic-Face-Detector-1MB](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) by Linzaer, a lightweight facedetection model designed for edge computing devices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_path = os.path.abspath(\"../data/facedetector/facedetector.onnx\")\n",
    "DOWNLOAD_URL = \"https://raw.githubusercontent.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/master/models/onnx/version-RFB-320.onnx\"\n",
    "if not os.path.isfile(dl_path):\n",
    "    urlretrieve(DOWNLOAD_URL, filename=dl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following content is based on the PyTorch tutorial [Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html), with modifications.\n",
    "\n",
    "We start by loading the model and running `onnx.checker.check_model` to check whether the model has a valid schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "model = onnx.load(dl_path)\n",
    "# model is a onnx.ModelProto object \n",
    "\n",
    "onnx.checker.check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize \n",
    "\n",
    "When loading some models, ONNX may return warnings that the model can be further optimized by removing some unused nodes. \n",
    "\n",
    "Use ONNX's optimizer to optimize your ONNX file. The code below is adapted from this [GitHub comment](https://github.com/microsoft/onnxruntime/issues/1899#issuecomment-534806537). \n",
    "\n",
    "Note that the API for optimizing models in ONNX Runtime is experimental, and [may change](https://github.com/onnx/onnx/blob/c08a7b76cf7c1555ae37186f12be4d62b2c39b3b/onnx/optimizer/optimize.h#L1-L2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(dl_path)\n",
    "passes = [\"extract_constant_to_initializer\", \"eliminate_unused_initializer\"]\n",
    "optimized_model = optimizer.optimize(onnx_model, passes)\n",
    "onnx.save(optimized_model, dl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python script with PyTorch and ONNX Runtime \n",
    "\n",
    "Now that we have an optimized ONNX file, we can serve our model. \n",
    "\n",
    "The following code:\n",
    "- transforms a [PIL](https://python-pillow.org/) image into a 240 x 320 image, \n",
    "- casts it into a PyTorch Tensor, \n",
    "- adds an extra dimension with [`unsqueeze`](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze), \n",
    "- casts the Tensor into a NumPy array, then \n",
    "- returns the model's output with ONNX Runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = \"\"\"\n",
    "\n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms\n",
    "import onnxruntime\n",
    "import os \n",
    "\n",
    "dl_path = os.path.abspath(\"../data/facedetector/facedetector.onnx\")\n",
    "\n",
    "image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "resize = transforms.Resize([240, 320])\n",
    "img_y = resize(image)\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_y = to_tensor(img_y)\n",
    "img_y.unsqueeze_(0)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(dl_path)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "_, boxes = ort_outs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the server\n",
    "\n",
    "### Defining a `PythonConfig` \n",
    "- Here we use the `python_code` argument instead of `python_code_path`, since the code is defined as a string. \n",
    "- Define the inputs and outputs as dictionaries, where the keys represent objects in the server's Python environment, and the values represent data types (Python data structures), defined as strings. See https://serving.oss.konduit.ai/python for supported data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = os.path.abspath('.')\n",
    "\n",
    "python_config = PythonConfig(\n",
    "    python_code=python_code,\n",
    "    python_inputs={\"image\": \"NDARRAY\"}, \n",
    "    python_outputs={\"boxes\": \"NDARRAY\"}, \n",
    "    python_path=default_python_path(work_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a pipeline step with the `PythonStep` class. \n",
    "\n",
    "In the `.step()` method, define a name for this step (`input1`) and the respective configuration (`python_config`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_step = (PythonStep()\n",
    "             .step(input_name=\"input1\", \n",
    "                   python_config=python_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the server configuration using the Server class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = np.random.randint(1000, 65535)\n",
    "\n",
    "server = Server(\n",
    "    steps=onnx_step, \n",
    "    serving_config=ServingConfig(http_port=port)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving the model \n",
    "\n",
    "Load a sample image using PIL/Pillow, start the server, and send the image to the server for prediction using the `predict()` method of the `Client` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server...\n",
      "\n",
      "Server has started successfully.\n"
     ]
    }
   ],
   "source": [
    "server.start()\n",
    "\n",
    "im = Image.open(\"../data/facedetector/1.jpg\")\n",
    "im = np.array(im).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Client\n",
    "Since the image is passed to the Server as a NumPy array, specify the input and output data format as `NUMPY`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 6.6688508e-03  6.6612866e-03  2.0925473e-02  3.1367622e-02]\n",
      "  [-8.1158802e-04 -7.6849163e-03  3.6023624e-02  4.8342105e-02]\n",
      "  [-9.6533615e-03 -1.9435773e-02  5.0060332e-02  6.8386555e-02]\n",
      "  ...\n",
      "  [ 7.3914373e-01  6.0356820e-01  1.0579867e+00  1.1062137e+00]\n",
      "  [ 6.6750574e-01  4.7108063e-01  1.1399301e+00  1.2245827e+00]\n",
      "  [ 5.8289015e-01  3.5682181e-01  1.1986192e+00  1.2655499e+00]]]\n"
     ]
    }
   ],
   "source": [
    "client = Client(port=port)\n",
    "\n",
    "output = client.predict(\n",
    "    {\"input1\": im}\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stop the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
