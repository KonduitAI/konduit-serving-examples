{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformations with Konduit and DataVec\n",
    "\n",
    "Konduit Serving supports data transformations defined by the DataVec vectorization and ETL library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konduit import TransformProcessStep, ServingConfig\n",
    "from konduit.server import Server\n",
    "from konduit.client import Client\n",
    "from konduit.utils import is_port_in_use\n",
    "\n",
    "from pydatavec import Schema, TransformProcess\n",
    "\n",
    "from utils import load_java_tp\n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataVec transformations can be defined in Python using the [PyDataVec](https://github.com/eclipse/deeplearning4j/tree/master/pydatavec) package, which can be installed from PyPi: \n",
    "\n",
    "``` \n",
    "pip install pydatavec\n",
    "```\n",
    "\n",
    "Using PyDataVec requires [Docker](https://docs.docker.com/v17.09/engine/installation/#supported-platforms). For Windows 10 Home edition users, note that Docker Toolbox is not supported. \n",
    "\n",
    "Run the following cell to check that your Docker installation is successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello from Docker!\n",
      "This message shows that your installation appears to be working correctly.\n",
      "\n",
      "To generate this message, Docker took the following steps:\n",
      " 1. The Docker client contacted the Docker daemon.\n",
      " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
      "    (amd64)\n",
      " 3. The Docker daemon created a new container from that image which runs the\n",
      "    executable that produces the output you are currently reading.\n",
      " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
      "    to your terminal.\n",
      "\n",
      "To try something more ambitious, you can run an Ubuntu container with:\n",
      " $ docker run -it ubuntu bash\n",
      "\n",
      "Share images, automate workflows, and more with a free Docker ID:\n",
      " https://hub.docker.com/\n",
      "\n",
      "For more examples and ideas, visit:\n",
      " https://docs.docker.com/get-started/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker run hello-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration \n",
    "\n",
    "## Server configuration \n",
    "\n",
    "### Data transformations \n",
    "\n",
    "#### `Schema` ([source](https://github.com/eclipse/deeplearning4j/blob/master/pydatavec/pydatavec/schema.py)) \n",
    "\n",
    "A `Schema` specifies the structure of your data. In DataVec, a `TransformProcess` requires the `Schema` of the data to be specified. \n",
    "\n",
    "`Schema` objects have a number of methods that define different data types for columns: `add_string_column()`, `add_integer_column()`, `add_long_column()`, `add_float_column()`, `add_double_column()` and `add_categorical_column()`. \n",
    "\n",
    "#### `TransformProcess` ([source](https://github.com/eclipse/deeplearning4j/blob/master/pydatavec/pydatavec/transform_process.py)) \n",
    "\n",
    "`TransformProcess` provides a number of methods to manipulate your data. The following methods are available in the Python API:\n",
    "\n",
    "- Reduce the number of rows: `filter()`\n",
    "- General data transformations: `replace()`, \n",
    "- Type casting: `string_to_time()`, `derive_column_from_time()`, `categorical_to_integer()`, \n",
    "- Combining/reducing the values in each column: `reduce()`\n",
    "- String operations: `append_string()`, `lower()`, `upper()`, `concat()`, `remove_white_spaces()`, `replace_empty_string()`, `replace_string()`, `map_string()`\n",
    "- Column selection/renaming: `remove()`, `remove_columns_except()`, `rename_column()`\n",
    "- One-hot encoding: `one_hot()`\n",
    "\n",
    "In this short example, we append the string `two` to the end of values in the string column `first`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema()\n",
    "schema.add_string_column(\"first\")\n",
    "\n",
    "tp = TransformProcess(schema)\n",
    "tp.append_string(\"first\", \"two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TransformProcess` configuration has to be converted into JSON format to be passed to Konduit Serving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_tp = tp.to_java()\n",
    "tp_json = java_tp.toJson()\n",
    "load_java_tp(tp_json)\n",
    "as_python_json = json.loads(tp_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline step\n",
    "\n",
    "The `TransformProcess` can now be defined in the Konduit Serving configuration with a `TransformProcessStep`. Here, we \n",
    "- **configure the inputs and outputs**: the schema, column names and data types should be defined here. \n",
    "- **declare the `TransformProcess`** using the `.transform_process()` method. \n",
    "\n",
    "Note that `Schema` data types are not defined in the same way as `PythonStep` data types. See the [source](https://github.com/KonduitAI/konduit-serving/blob/78851701004ebb3dbf079889d46b79a9db8fac60/konduit-serving-api/src/main/java/ai/konduit/serving/util/SchemaTypeUtils.java#L154-L195) for a complete list of supported Schema data types: \n",
    "- `NDArray`\n",
    "- `String`\n",
    "- `Boolean`\n",
    "- `Categorical`\n",
    "- `Float`\n",
    "- `Double`\n",
    "- `Integer`\n",
    "- `Long`\n",
    "- `Bytes`\n",
    "\n",
    "You should define the Schema data types in `TransformProcessStep()` as strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_step = (TransformProcessStep()\n",
    "                  .set_input(schema=None, \n",
    "                             column_names=[\"first\"], \n",
    "                             types=[\"String\"])\n",
    "                  .set_output(schema=None, \n",
    "                              column_names=[\"first\"], \n",
    "                              types=[\"String\"])\n",
    "                  .transform_process(as_python_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring ports and data formats\n",
    "\n",
    "Configure the Server using `ServingConfig` to define the port using the `http_port` argument and data formats using the `input_data_type` and `output_data_type` arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = np.random.randint(1000, 65535)\n",
    "serving_config = ServingConfig(\n",
    "    http_port=port,\n",
    "    input_data_format='JSON',\n",
    "    output_data_format='JSON',\n",
    ")\n",
    "\n",
    "server = Server(\n",
    "    serving_config=serving_config,\n",
    "    steps=[transform_step]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete configuration is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@type': 'InferenceConfiguration',\n",
       " 'steps': [{'@type': 'TransformProcessStep',\n",
       "   'inputSchemas': {'default': ['String']},\n",
       "   'outputSchemas': {'default': ['String']},\n",
       "   'inputNames': ['default'],\n",
       "   'outputNames': ['default'],\n",
       "   'inputColumnNames': {'default': ['first']},\n",
       "   'outputColumnNames': {'default': ['first']},\n",
       "   'transformProcesses': {'default': {'actionList': [{'transform': {'@class': 'org.datavec.api.transform.transform.string.AppendStringColumnTransform',\n",
       "        'columnName': 'first',\n",
       "        'toAppend': 'two'}}],\n",
       "     'initialSchema': {'@class': 'org.datavec.api.transform.schema.Schema',\n",
       "      'columns': [{'@class': 'org.datavec.api.transform.metadata.StringMetaData',\n",
       "        'name': 'first'}]}}}}],\n",
       " 'servingConfig': {'@type': 'ServingConfig',\n",
       "  'httpPort': 33114,\n",
       "  'inputDataFormat': 'JSON',\n",
       "  'outputDataFormat': 'JSON',\n",
       "  'logTimings': True}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.config.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Client\n",
    "\n",
    "The `Client` should be configured to match the Konduit Serving instance. \n",
    "- `input_names` and `output_names` should match the columns in the DataVec schema \n",
    "- Data formats should be defined as one of the following: `JSON`, `RAW`, `ARROW`, `IMAGE`, `NUMPY` \n",
    "- As this example is run on a local computer, the server is located at host `'http://localhost'` and port `port`. Since `'http://localhost'` is the default argument, we can omit it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server..\n",
      "\n",
      "Server has started successfully.\n"
     ]
    }
   ],
   "source": [
    "server.start()\n",
    "\n",
    "client = Client(input_names=[\"first\"],\n",
    "                output_names=[\"first\"],\n",
    "                return_output_data_format='JSON',\n",
    "                input_data_format='JSON',\n",
    "                output_data_format='RAW',\n",
    "                port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the server\n",
    "\n",
    "Finally, we run the Konduit Serving instance. Recall that the `TransformProcessStep()` appends a string `two` to strings in the column `first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first': 'valuetwo'}\n"
     ]
    }
   ],
   "source": [
    "data_input = {'first': 'value'}\n",
    "predicted = client.predict(data_input)\n",
    "print(predicted)\n",
    "server.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
