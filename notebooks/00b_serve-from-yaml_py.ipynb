{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konduit Serving Model Runtime with YAML Configuration\n",
    "\n",
    "Konduit supports specifying server configurations as YAML files. This allows you to serve simple server configurations using: \n",
    "1. the Konduit Python CLI, and   \n",
    "2. the `konduit.load` module. \n",
    "\n",
    "The YAMLs on this page can be used as boilerplate code for your model serving use cases. \n",
    "\n",
    "Some resources on the YAML format are as follows: \n",
    "- https://gettaurus.org/docs/YAMLTutorial/\n",
    "- https://docs.saltstack.com/en/latest/topics/yaml/\n",
    "- http://jessenoller.com/blog/2009/04/13/yaml-aint-markup-language-completely-different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import numpy as np \n",
    "import os\n",
    "from konduit.load import server_from_file, client_from_file\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve \n",
    "from zipfile import ZipFile\n",
    "dl_path = \"../data/bert/bert.zip\"\n",
    "if not os.path.isfile(dl_path):\n",
    "    urlretrieve(\"https://deeplearning4jblob.blob.core.windows.net/testresources/bert_mrpc_frozen_v1.zip\", \n",
    "                dl_path)\n",
    "with ZipFile(dl_path, 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Konduit Serving YAML configuration file has three top-level entities: \n",
    "1. `serving`\n",
    "2. `steps`\n",
    "2. `client`\n",
    "\n",
    "The following is a sample YAML file for serving a Python script located at `simple.py` which takes a NumPy array `first` as input and a NumPy array `second` as output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  python_step:\n",
    "    type: PYTHON\n",
    "    python_code_path: ./simple.py\n",
    "    python_inputs:\n",
    "      first: NDARRAY\n",
    "    python_outputs:\n",
    "      second: NDARRAY\n",
    "client:\n",
    "    port: 1337\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving \n",
    "\n",
    "The server configuration takes the following arguments: \n",
    "\n",
    "- `http_port`: specify the port number \n",
    "- `input_data_format` and `output_data_format`: specify one of the following: JSON, NUMPY, ARROW, IMAGE\n",
    "- `log_timings`: specify True to log timings \n",
    "- `extra_start_args`: Java Virtual Machine (JVM) arguments. In this case, `-Xmx8g` specifies that the maximum memory allocation for the JVM is 8GB. \n",
    "\n",
    "\n",
    "Refer to the [Server](https://serving.oss.konduit.ai/server/inference) documentation for details. \n",
    "\n",
    "\n",
    "## Client \n",
    "\n",
    "Refer to the [Client](https://serving.oss.konduit.ai/client/python-client) documentation for details. \n",
    "\n",
    "- `input_names`, `output_names`: names of the first and final nodes of the Konduit Serving pipeline configuration defined in the Server. These arguments are typically inherited from the Server when initialized. \n",
    "- `input_data_format`, `output_data_format`, `return_output_data_format`: One of the following: JSON, NUMPY, ARROW, IMAGE. `input_data_format` and `output_data_format` refer to the format of the server's input and output, whereas `return_output_data_format` specifies the data format returned to the client. \n",
    "- `port`: specify the same HTTP port as the Server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running YAML configurations \n",
    "\n",
    "A Konduit Serving instance can be started using \n",
    "\n",
    "```python\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "```\n",
    "\n",
    "assuming the path of your YAML configuration is specified in `konduit_yaml_path`. \n",
    "\n",
    "Note that the file also contains Client configuration. To create a Client object, run the following code: \n",
    "\n",
    "```python\n",
    "client = client_from_file(konduit_yaml_path)\n",
    "```\n",
    "\n",
    "Now your data can be passed to `client` for prediction using: \n",
    "\n",
    "```python\n",
    "client.predict(data_input)\n",
    "```\n",
    "\n",
    "where `data_input` is a dictionary. \n",
    "\n",
    "If the input name is `default`, a NumPy array can be directly passed to the `.predict()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonStep\n",
    "\n",
    "Python steps can take any argument that can be passed to `PythonConfig`.  \n",
    "Specify a Python step as follows: \n",
    "\n",
    "```yaml\n",
    "steps: \n",
    "  python_step: \n",
    "    type: PYTHON\n",
    "    python_code: simple.py \n",
    "```\n",
    "\n",
    "- `type`: specify this as PYTHON\n",
    "- `python_code`: if you want to specify your Python code directly in your YAML file. The following [documentation](http://blogs.perl.org/users/tinita/2018/03/strings-in-yaml---to-quote-or-not-to-quote.html) may be helpful for specifying multi-line Python code, specifically the section on literal block scalars.\n",
    "- `python_code_path`: specify the path of a Python `.py` script. \n",
    "- `python_inputs`: name-value pairs specifying the data types for each of the inputs referenced in the script \n",
    "- `python_outputs`: name-value pairs specifying the data types for each of the outputs referenced in the script\n",
    "- `python_path`: location of the Python modules. Generally, if your script only requires NumPy, setting a custom `python_path` is not necessary. Refer to the [Python modules](https://serving.oss.konduit.ai/python#python-modules-and-the-pythonpath-argument) documentation on setting a custom Python path with additional modules. \n",
    "\n",
    "The names referenced in `python_inputs` and `python_outputs` correspond with `inputColumnNames` and `outputColumnNames`.  Modifying `python_inputs` and `python_outputs` does not modify the input and output name of the step. `input_names` and `output_names` are arguments to `PythonStep` which cannot be accessed through the YAML configuration, and default to the name `default`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Array operation\n",
    "\n",
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  python_step:\n",
    "    type: PYTHON\n",
    "    python_code_path: ./simple.py\n",
    "    python_inputs:\n",
    "      first: NDARRAY\n",
    "    python_outputs:\n",
    "      second: NDARRAY\n",
    "client:\n",
    "    port: 1337\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server....\n",
      "\n",
      "Server has started successfully.\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "input_arr = np.array(33)\n",
    "\n",
    "konduit_yaml_path = \"../yaml/simple.yaml\"\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "client = client_from_file(konduit_yaml_path)\n",
    "print(client.predict(input_arr))\n",
    "server.stop()\n",
    "\n",
    "np.save(\"../data/simple/input_arr.npy\", input_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: PyTorch and ONNX Runtime\n",
    "\n",
    "The following is a sample YAML for serving a PyTorch model using a Python script.\n",
    "\n",
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  python_step:\n",
    "    type: PYTHON\n",
    "    python_path: .\n",
    "    python_code_path: ../python/pytorch.py\n",
    "    python_inputs:\n",
    "      image: NDARRAY\n",
    "    python_outputs:\n",
    "      img_out_y: NDARRAY\n",
    "client:\n",
    "    port: 1337\n",
    "```\n",
    "\n",
    "Note the following: \n",
    "- The `python_path` has been left out intentionally. Replace this following the instructions in the [Python modules documentation](https://serving.oss.konduit.ai/python#python-modules-and-the-pythonpath-argument), making sure that ONNX and PyTorch are installed in said Python environment. Refer to the [PyTorch quickstart](https://pytorch.org/).\n",
    "- The YAML file referenced below shows how to use YAML literal blocks to embed the Python code within your YAML file. \n",
    "\n",
    "Refer to the [ONNX Runtime](https://serving.oss.konduit.ai/examples/onnx) page for complete documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server..\n",
      "\n",
      "Server has started successfully.\n",
      "[[[ 0.00601701  0.00688479  0.02177745  0.03408115]\n",
      "  [-0.0018133  -0.00657785  0.03698186  0.05206966]\n",
      "  [-0.01035942 -0.01786287  0.04902049  0.06799769]\n",
      "  ...\n",
      "  [ 0.7294515   0.6165271   1.0584102   1.1059598 ]\n",
      "  [ 0.65046376  0.48442802  1.141786    1.2248938 ]\n",
      "  [ 0.5633501   0.37209463  1.2047783   1.2747201 ]]]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"../data/facedetector/1.jpg\")\n",
    "im = np.array(im).astype(\"int\")\n",
    "\n",
    "konduit_yaml_path = \"../yaml/pytorch.yaml\"\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "client = client_from_file(konduit_yaml_path)\n",
    "print(client.predict(im))\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelStep\n",
    "\n",
    "## Deeplearning4j\n",
    "A Deeplearning4j model step can be specified as follows: \n",
    "\n",
    "```yaml\n",
    "steps:\n",
    "  dl4j_mln_step:\n",
    "    type: MULTI_LAYER_NETWORK\n",
    "    model_loading_path: ../data/multilayernetwork/SimpleCNN.zip\n",
    "    input_names: \n",
    "    - image_array\n",
    "    output_names: \n",
    "    - output\n",
    "    input_data_types:\n",
    "      image_array: FLOAT\n",
    "```\n",
    "\n",
    "Depending on the type of model, specify a `dl4j_mln_step` or a `dl4j_cg_step` for MultiLayerNetwork and ComputationGraph models respectively. \n",
    "\n",
    "\n",
    "- `type`: `MULTI_LAYER_NETWORK` or `COMPUTATION_GRAPH`\n",
    "- `model_loading_path`: location of model weights \n",
    "- `input_names` and `output_names`: name of input and output nodes. See [here](https://serving.oss.konduit.ai/examples/dl4j#configuring-modelstep) for details on obtaining the names of input and output nodes. \n",
    "- `input_data_types`: map input nodes to data types. List of accepted data types are available [here](https://github.com/KonduitAI/konduit-serving/blob/master/konduit-serving-api/src/main/java/ai/konduit/serving/model/TensorDataType.java).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a sample YAML file for serving a Deeplearning4j model:\n",
    "\n",
    "```yaml\n",
    "steps:\n",
    "  dl4j_mln_step:\n",
    "    type: MULTI_LAYER_NETWORK\n",
    "    model_loading_path: ../data/multilayernetwork/SimpleCNN.zip\n",
    "    input_names: \n",
    "    - image_array\n",
    "    output_names: \n",
    "    - output\n",
    "    input_data_types:\n",
    "      image_array: FLOAT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server...\n",
      "\n",
      "Server has started successfully.\n",
      "[[4.1334968e-02 3.2425719e-01 2.5129888e-02 3.9935028e-05 6.0923803e-01]]\n"
     ]
    }
   ],
   "source": [
    "rand_image = np.random.randint(255, size=(1, 3, 224, 224)) / 255\n",
    "konduit_yaml_path = \"../yaml/deeplearning4j.yaml\"\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "client = client_from_file(konduit_yaml_path)\n",
    "print(client.predict({\"image_array\": rand_image}))\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph ('frozen model')\n",
    "\n",
    "Konduit Serving supports loading models saved in the TensorFlow Graph format. See the [relevant documentation](https://serving.oss.konduit.ai/examples/tensorflow-model-serving/tf-mnist) on how to save models in the TensorFlow Graph format. \n",
    "\n",
    "Declare a TensorFlow step in your YAML file as follows: \n",
    "\n",
    "- `type`: set `type` as `TENSORFLOW`\n",
    "- `model_loading_path`: location of the model weights \n",
    "- `input_names`, `output_names`: a list of the input and output nodes \n",
    "- `input_data_types`: maps input nodes to the corresponding [data type](https://github.com/KonduitAI/konduit-serving/blob/master/konduit-serving-api/src/main/java/ai/konduit/serving/model/TensorDataType.java)\n",
    "- `parallel_inference_config`: specify the number of workers to run in parallel \n",
    "\n",
    "### Example 1: MNIST classifier\n",
    "```yaml\n",
    "steps:\n",
    "  tensorflow_step:\n",
    "    type: TENSORFLOW\n",
    "    model_loading_path: ../data/mnist/mnist_2.0.0.pb\n",
    "    input_names:\n",
    "      - input_layer\n",
    "    output_names:\n",
    "      - output_layer/Softmax\n",
    "    input_data_types:\n",
    "      input_layer: FLOAT\n",
    "    parallel_inference_config:\n",
    "      workers: 1\n",
    "```\n",
    "\n",
    "We can start a Konduit Serving instance as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server....\n",
      "\n",
      "Server has started successfully.\n"
     ]
    }
   ],
   "source": [
    "img = np.load(\"../data/mnist/input_layer.npy\")\n",
    "konduit_yaml_path = \"../yaml/tensorflow-mnist.yaml\"\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "client = client_from_file(konduit_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 0.0, 2: 0.001, 3: 0.001, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.998, 8: 0.0, 9: 0.0}\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(img)\n",
    "predicted = client.predict(\n",
    "    data_input={'input_layer': np.expand_dims(img, axis=0)}\n",
    ")\n",
    "plt.show()\n",
    "print(dict(zip(np.arange(10), predicted[0].round(3))))\n",
    "    \n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multiple input nodes \n",
    "\n",
    "A sample YAML serving a TensorFlow Graph model with multiple input nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml \n",
    "steps:\n",
    "  tensorflow_step:\n",
    "    type: TENSORFLOW\n",
    "    model_loading_path: bert_mrpc_frozen.pb\n",
    "    input_names:\n",
    "      - IteratorGetNext:0\n",
    "      - IteratorGetNext:1\n",
    "      - IteratorGetNext:4\n",
    "    output_names:\n",
    "      - loss/Softmax\n",
    "    input_data_types:\n",
    "      IteratorGetNext:0: INT32\n",
    "      IteratorGetNext:1: INT32\n",
    "      IteratorGetNext:4: INT32\n",
    "    parallel_inference_config:\n",
    "      workers: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server.........................\n",
      "\n",
      "Server has started successfully.\n",
      "[[9.9858320e-01 1.4168088e-03]\n",
      " [8.8414288e-04 9.9911588e-01]\n",
      " [9.9859291e-01 1.4070126e-03]\n",
      " [9.9752253e-01 2.4774617e-03]]\n"
     ]
    }
   ],
   "source": [
    "konduit_yaml_path = \"../yaml/tensorflow-bert.yaml\"\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "client = client_from_file(konduit_yaml_path)\n",
    "\n",
    "data_input = {\n",
    "    'IteratorGetNext:0': np.expand_dims(np.load('../data/bert/input-0.npy'), axis=0),\n",
    "    'IteratorGetNext:1': np.expand_dims(np.load('../data/bert/input-1.npy'), axis=0),\n",
    "    'IteratorGetNext:4': np.expand_dims(np.load('../data/bert/input-4.npy'), axis=0)\n",
    "}\n",
    "\n",
    "print(client.predict(data_input))\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras \n",
    "\n",
    "Konduit Serving supports Keras HDF5 models via Deeplearning4J model import. The following is a sample YAML file for serving a Keras model: \n",
    "\n",
    "- `type`: specify this as `KERAS`\n",
    "- `model_loading_path`: location of the model weights \n",
    "- `input_names`, `output_names`: names for the input and output nodes, as lists  \n",
    "\n",
    "Input and output names can be obtained by visualizing the graph in [Netron](https://github.com/lutzroeder/netron). \n",
    "\n",
    "```yaml\n",
    "steps:\n",
    "  keras_step:\n",
    "    type: KERAS\n",
    "    model_loading_path: ../data/keras/embedding_lstm_tensorflow_2.h5\n",
    "    input_names:\n",
    "    - input \n",
    "    output_names:\n",
    "    - lstm_1\n",
    "\n",
    "```\n",
    "\n",
    "This YAML can be served using the `konduit.load` module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server...\n",
      "\n",
      "Server has started successfully.\n",
      "[[[-3.53171281e-03 -6.55398145e-03 -9.07615386e-03 -1.11385630e-02\n",
      "   -1.27962595e-02 -1.41087854e-02 -1.51341194e-02 -1.59251783e-02\n",
      "   -1.65283810e-02 -1.69831831e-02]\n",
      "  [-8.98816856e-04 -2.16921209e-03 -3.50447348e-03 -4.74313367e-03\n",
      "   -5.81311202e-03 -6.69444632e-03 -7.39536854e-03 -7.93745089e-03\n",
      "   -8.34691338e-03 -8.64967890e-03]\n",
      "  [ 4.57964110e-04  7.27277948e-04  8.35937157e-04  8.26211413e-04\n",
      "    7.38725299e-04  6.06592686e-04  4.54327965e-04  2.98691681e-04\n",
      "    1.50312349e-04  1.51993027e-05]\n",
      "  [-3.72108625e-04 -7.85349868e-04 -1.15279201e-03 -1.44809973e-03\n",
      "   -1.67194800e-03 -1.83526019e-03 -1.95118226e-03 -2.03184737e-03\n",
      "   -2.08712765e-03 -2.12461245e-03]\n",
      "  [-1.70938822e-03 -2.67439662e-03 -3.14217363e-03 -3.29596084e-03\n",
      "   -3.26494290e-03 -3.13617825e-03 -2.96555483e-03 -2.78691365e-03\n",
      "   -2.61911890e-03 -2.47139740e-03]\n",
      "  [ 4.90737101e-03  8.10048729e-03  1.01506319e-02  1.14426687e-02\n",
      "    1.22359265e-02  1.27047608e-02  1.29658673e-02  1.30968681e-02\n",
      "    1.31488722e-02  1.31550385e-02]]]\n"
     ]
    }
   ],
   "source": [
    "input_array = np.random.uniform(size = [10])\n",
    "np.save(\"../data/keras/input.npy\", input_array)\n",
    "konduit_yaml_path = \"../yaml/keras.yaml\"\n",
    "server = server_from_file(konduit_yaml_path)\n",
    "client = client_from_file(konduit_yaml_path)\n",
    "print(client.predict({\"input\": np.expand_dims(input_array, axis=0)}))\n",
    "\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
