{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konduit Serving Model Runtime with YAML Configuration\n",
    "\n",
    "Konduit supports specifying server configurations as YAML files. This allows you to serve simple server configurations using: \n",
    "1. the Konduit Python CLI, and   \n",
    "2. the `konduit.load` module. \n",
    "\n",
    "The YAMLs on this page can be used as boilerplate code for your model serving use cases. \n",
    "\n",
    "Some resources on the YAML format are as follows: \n",
    "- https://gettaurus.org/docs/YAMLTutorial/\n",
    "- https://docs.saltstack.com/en/latest/topics/yaml/\n",
    "- http://jessenoller.com/blog/2009/04/13/yaml-aint-markup-language-completely-different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve \n",
    "from zipfile import ZipFile\n",
    "dl_path = \"../data/bert/bert.zip\"\n",
    "if not os.path.isfile(dl_path):\n",
    "    urlretrieve(\"https://deeplearning4jblob.blob.core.windows.net/testresources/bert_mrpc_frozen_v1.zip\", \n",
    "                dl_path)\n",
    "with ZipFile(dl_path, 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Konduit Serving YAML configuration file has three top-level entities: \n",
    "1. `serving`\n",
    "2. `steps`\n",
    "2. `client`\n",
    "\n",
    "The following is a sample YAML file for serving a Python script located at `simple.py` which takes a NumPy array `first` as input and a NumPy array `second` as output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  python_step:\n",
    "    type: PYTHON\n",
    "    python_path: .\n",
    "    python_code_path: ./simple.py\n",
    "    python_inputs:\n",
    "      first: NDARRAY\n",
    "    python_outputs:\n",
    "      second: NDARRAY\n",
    "client:\n",
    "    port: 1337\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving \n",
    "\n",
    "The server configuration takes the following arguments: \n",
    "\n",
    "- `http_port`: specify the port number \n",
    "- `input_data_format` and `output_data_format`: specify one of the following: JSON, NUMPY, ARROW, IMAGE\n",
    "- `log_timings`: specify True to log timings \n",
    "- `extra_start_args`: Java Virtual Machine (JVM) arguments. In this case, `-Xmx8g` specifies that the maximum memory allocation for the JVM is 8GB. \n",
    "\n",
    "\n",
    "Refer to the [Server](https://serving.oss.konduit.ai/server/inference) documentation for details. \n",
    "\n",
    "\n",
    "## Client \n",
    "\n",
    "Refer to the [Client](https://serving.oss.konduit.ai/client/python-client) documentation for details. \n",
    "\n",
    "- `input_names`, `output_names`: names of the first and final nodes of the Konduit Serving pipeline configuration defined in the Server. These arguments are typically inherited from the Server when initialized. \n",
    "- `input_data_format`, `output_data_format`, `return_output_data_format`: One of the following: JSON, NUMPY, ARROW, IMAGE. `input_data_format` and `output_data_format` refer to the format of the server's input and output, whereas `return_output_data_format` specifies the data format returned to the client. \n",
    "- `port`: specify the same HTTP port as the Server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running YAML configurations \n",
    "\n",
    "Assume all commands are run from this folder (`notebooks`). \n",
    "\n",
    "The CLI provides a handy command `predict-numpy` that returns predictions from a model server, if the input name is `default` and a **NumPy array** is supplied as input. To initialize the server, run the following command: \n",
    "\n",
    "```bash \n",
    "konduit serve --config ../yaml/konduit.yaml\n",
    "```\n",
    "\n",
    "Once the server has started, run `predict-numpy` to obtain the predicted output given the location of the NumPy array saved as a [NumPy `.npy` file](https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.format.html): \n",
    "\n",
    "```bash\n",
    "konduit predict-numpy --config ../yaml/konduit.yaml --numpy_data ../data/bert/input-0.npy\n",
    "```\n",
    "\n",
    "Finally, to stop the server, run the `stop-server` command: \n",
    "```bash\n",
    "konduit stop-server --config ../yaml/konduit.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonStep\n",
    "\n",
    "Python steps can take any argument that can be passed to `PythonConfig`.  \n",
    "Specify a Python step as follows: \n",
    "\n",
    "```yaml\n",
    "steps: \n",
    "  python_step: \n",
    "    type: PYTHON\n",
    "    python_code: simple.py \n",
    "```\n",
    "\n",
    "- `type`: specify this as PYTHON\n",
    "- `python_code`: if you want to specify your Python code directly in your YAML file. The following [documentation](http://blogs.perl.org/users/tinita/2018/03/strings-in-yaml---to-quote-or-not-to-quote.html) may be helpful for specifying multi-line Python code, specifically the section on literal block scalars.\n",
    "- `python_code_path`: specify the path of a Python `.py` script. \n",
    "- `python_inputs`: name-value pairs specifying the data types for each of the inputs referenced in the script \n",
    "- `python_outputs`: name-value pairs specifying the data types for each of the outputs referenced in the script\n",
    "- `python_path`: location of the Python modules. Generally, if your script only requires NumPy, setting a custom `python_path` is not necessary. Refer to the [Python modules](https://serving.oss.konduit.ai/python#python-modules-and-the-pythonpath-argument) documentation on setting a custom Python path with additional modules. \n",
    "\n",
    "The names referenced in `python_inputs` and `python_outputs` correspond with `inputColumnNames` and `outputColumnNames`.  Modifying `python_inputs` and `python_outputs` does not modify the input and output name of the step. `input_names` and `output_names` are arguments to `PythonStep` which cannot be accessed through the YAML configuration, and default to the name `default`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Array operation\n",
    "\n",
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  python_step:\n",
    "    type: PYTHON\n",
    "    python_code_path: ./simple.py\n",
    "    python_inputs:\n",
    "      first: NDARRAY\n",
    "    python_outputs:\n",
    "      second: NDARRAY\n",
    "client:\n",
    "    port: 1337\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the command line: \n",
    "\n",
    "```bash\n",
    "konduit serve --config ../yaml/simple.yaml\n",
    "konduit predict-numpy --config ../yaml/simple.yaml --numpy_data ../data/simple/input_arr.npy\n",
    "konduit stop-server --config ../yaml/simple.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: PyTorch and ONNX Runtime\n",
    "\n",
    "The following is a sample YAML for serving a PyTorch model using a Python script.\n",
    "\n",
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  python_step:\n",
    "    type: PYTHON\n",
    "    python_path: .\n",
    "    python_code_path: ../python/pytorch.py\n",
    "    python_inputs:\n",
    "      image: NDARRAY\n",
    "    python_outputs:\n",
    "      img_out_y: NDARRAY\n",
    "client:\n",
    "    port: 1337\n",
    "```\n",
    "\n",
    "Note the following: \n",
    "- The `python_path` has been left out intentionally. Replace this following the instructions in the [Python modules documentation](https://serving.oss.konduit.ai/python#python-modules-and-the-pythonpath-argument), making sure that ONNX and PyTorch are installed in said Python environment. Refer to the [PyTorch quickstart](https://pytorch.org/). \n",
    "- The YAML file referenced below shows how to use YAML literal blocks to embed the Python code within your YAML file. \n",
    "\n",
    "Refer to the [ONNX Runtime](https://serving.oss.konduit.ai/examples/onnx) page for complete documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the command line: \n",
    "\n",
    "```bash\n",
    "konduit serve --config ../yaml/pytorch.yaml\n",
    "konduit predict-numpy --config ../yaml/pytorch.yaml --numpy_data ../data/pytorch/im.npy\n",
    "konduit stop-server --config ../yaml/pytorch.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelStep\n",
    "\n",
    "## Deeplearning4j\n",
    "A Deeplearning4j model step can be specified as follows: \n",
    "\n",
    "```yaml\n",
    "steps:\n",
    "  dl4j_mln_step:\n",
    "    type: MULTI_LAYER_NETWORK\n",
    "    model_loading_path: ../data/multilayernetwork/SimpleCNN.zip\n",
    "    input_names: \n",
    "    - image_array\n",
    "    output_names: \n",
    "    - output\n",
    "    input_data_types:\n",
    "      image_array: FLOAT\n",
    "```\n",
    "\n",
    "Depending on the type of model, specify a `dl4j_mln_step` or a `dl4j_cg_step` for MultiLayerNetwork and ComputationGraph models respectively. \n",
    "\n",
    "\n",
    "- `type`: `MULTI_LAYER_NETWORK` or `COMPUTATION_GRAPH`\n",
    "- `model_loading_path`: location of model weights \n",
    "- `input_names` and `output_names`: name of input and output nodes. See [here](https://serving.oss.konduit.ai/examples/dl4j#configuring-modelstep) for details on obtaining the names of input and output nodes. \n",
    "- `input_data_types`: map input nodes to data types. List of accepted data types are available [here](https://github.com/KonduitAI/konduit-serving/blob/master/konduit-serving-api/src/main/java/ai/konduit/serving/model/TensorDataType.java).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a sample YAML file for serving a Deeplearning4j model:\n",
    "\n",
    "```yaml\n",
    "serving:\n",
    "  http_port: 1337\n",
    "  input_data_format: NUMPY\n",
    "  output_data_format: NUMPY\n",
    "  log_timings: True\n",
    "  extra_start_args: -Xmx8g\n",
    "steps:\n",
    "  dl4j_mln_step:\n",
    "    type: MULTI_LAYER_NETWORK\n",
    "    model_loading_path: ../data/multilayernetwork/SimpleCNN.zip\n",
    "    input_names: \n",
    "    - image_array\n",
    "    output_names: \n",
    "    - output\n",
    "    input_data_types:\n",
    "      image_array: FLOAT\n",
    "client:\n",
    "    port: 1337\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the command line: \n",
    "\n",
    "```bash\n",
    "konduit serve --config ../yaml/deeplearning4j.yaml\n",
    "konduit predict-numpy --config ../yaml/deeplearning4j.yaml --numpy_data ../data/multilayernetwork/image_array.npy --input_names input_layer\n",
    "konduit stop-server --config ../yaml/deeplearning4j.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph ('frozen model')\n",
    "\n",
    "Konduit Serving supports loading models saved in the TensorFlow Graph format. See the [relevant documentation](https://serving.oss.konduit.ai/examples/tensorflow-model-serving/tf-mnist) on how to save models in the TensorFlow Graph format. \n",
    "\n",
    "Declare a TensorFlow step in your YAML file as follows: \n",
    "\n",
    "- `type`: set `type` as `TENSORFLOW`\n",
    "- `model_loading_path`: location of the model weights \n",
    "- `input_names`, `output_names`: a list of the input and output nodes \n",
    "- `input_data_types`: maps input nodes to the corresponding [data type](https://github.com/KonduitAI/konduit-serving/blob/master/konduit-serving-api/src/main/java/ai/konduit/serving/model/TensorDataType.java)\n",
    "- `parallel_inference_config`: specify the number of workers to run in parallel \n",
    "\n",
    "### Example 1: MNIST classifier\n",
    "```yaml\n",
    "steps:\n",
    "  tensorflow_step:\n",
    "    type: TENSORFLOW\n",
    "    model_loading_path: ../data/mnist/mnist_2.0.0.pb\n",
    "    input_names:\n",
    "      - input_layer\n",
    "    output_names:\n",
    "      - output_layer/Softmax\n",
    "    input_data_types:\n",
    "      input_layer: FLOAT\n",
    "    parallel_inference_config:\n",
    "      workers: 1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the command line: \n",
    "\n",
    "```bash\n",
    "konduit serve --config ../yaml/tensorflow-mnist.yaml\n",
    "konduit predict-numpy --config ../yaml/tensorflow-mnist.yaml --numpy_data ../data/mnist/input_layer.npy --input_names input_layer\n",
    "konduit stop-server --config ../yaml/tensorflow-mnist.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multiple input nodes \n",
    "\n",
    "A sample YAML serving a TensorFlow Graph model with multiple input nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml \n",
    "steps:\n",
    "  tensorflow_step:\n",
    "    type: TENSORFLOW\n",
    "    model_loading_path: bert_mrpc_frozen.pb\n",
    "    input_names:\n",
    "      - IteratorGetNext:0\n",
    "      - IteratorGetNext:1\n",
    "      - IteratorGetNext:4\n",
    "    output_names:\n",
    "      - loss/Softmax\n",
    "    input_data_types:\n",
    "      IteratorGetNext:0: INT32\n",
    "      IteratorGetNext:1: INT32\n",
    "      IteratorGetNext:4: INT32\n",
    "    parallel_inference_config:\n",
    "      workers: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the command line: \n",
    "\n",
    "```bash\n",
    "konduit serve --config ../yaml/tensorflow-bert.yaml\n",
    "konduit predict-numpy --config ../yaml/tensorflow-bert.yaml --numpy_data \"../data/bert/input-0.npy,../data/bert/input-1.npy,../data/bert/input-4.npy\" --input_names \"IteratorGetNext:0,IteratorGetNext:1,IteratorGetNext:4\"\n",
    "konduit stop-server --config ../yaml/tensorflow-bert.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras \n",
    "\n",
    "Konduit Serving supports Keras HDF5 models via Deeplearning4J model import. The following is a sample YAML file for serving a Keras model: \n",
    "\n",
    "- `type`: specify this as `KERAS`\n",
    "- `model_loading_path`: location of the model weights \n",
    "- `input_names`, `output_names`: names for the input and output nodes, as lists  \n",
    "\n",
    "Input and output names can be obtained by visualizing the graph in [Netron](https://github.com/lutzroeder/netron). \n",
    "\n",
    "```yaml\n",
    "steps:\n",
    "  keras_step:\n",
    "    type: KERAS\n",
    "    model_loading_path: ../data/keras/embedding_lstm_tensorflow_2.h5\n",
    "    input_names:\n",
    "    - input \n",
    "    output_names:\n",
    "    - lstm_1\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the command line: \n",
    "\n",
    "```bash\n",
    "konduit serve --config ../yaml/keras.yaml\n",
    "konduit predict-numpy --config ../yaml/keras.yaml --numpy_data ../data/keras/input.npy --input_names input\n",
    "konduit stop-server --config ../yaml/keras.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
