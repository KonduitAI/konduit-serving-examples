{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve a Keras model with Konduit Serving \n",
    "\n",
    "This notebook illustrates a simple client-server interaction to perform inference on a Keras LSTM model using the Python SDK for Konduit Serving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konduit import ModelConfig, ParallelInferenceConfig, ModelConfigType, \\\n",
    "ModelStep, ServingConfig\n",
    "\n",
    "from konduit.server import Server\n",
    "from konduit.client import Client\n",
    "import os \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving models in Keras H5 format\n",
    "\n",
    "HDF5 model files can be saved with the `.save()` method. Refer to the [TensorFlow documentation for Keras](https://www.tensorflow.org/guide/keras/save_and_serialize) for details. \n",
    "\n",
    "Note: Keras model loading functionality in Konduit Serving converts Keras models to Deeplearning4J models. As a result, Keras models containing operations not supported in Deeplearning4J cannot be served in Konduit Serving. See [issue 8348](https://github.com/eclipse/deeplearning4j/issues/8348). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the server\n",
    "\n",
    "Konduit Serving works by defining a series of **steps**. These include operations such as \n",
    "1. Pre- or post-processing steps\n",
    "2. One or more machine learning models\n",
    "3. Transforming the output in a way that can be understood by humans\n",
    "\n",
    "If deploying your model does not require pre- nor post-processing, only one step - a machine learning model - is required. This configuration is defined using a single `ModelStep`. \n",
    "\n",
    "Before running this notebook, run the `build_jar.py` script and copy the JAR (`konduit.jar`) to this folder. Refer to the [Python SDK README](https://github.com/KonduitAI/konduit-serving/blob/master/python/README.md) for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring step \n",
    "\n",
    "Define the Keras configuration as a `ModelConfig` object. \n",
    "- `model_config_type`: This argument requires a `ModelConfigType` object. Specify `model_type` as `KERAS`, and `model_loading_path` to point to the location of Keras weights saved in the HDF5 file format.\n",
    "\n",
    "For the `ModelStep` object, the  following parameters are specified: \n",
    "- `model_config`: pass the ModelConfig object here \n",
    "- `parallel_inference_config`: specify the number of workers to run in parallel. Here, we specify `workers = 1`.\n",
    "- `input_names`:  names for the input nodes  \n",
    "- `output_names`: names for the output nodes\n",
    "\n",
    "Input and output names can be obtained by visualizing the graph in [Netron](https://github.com/lutzroeder/netron). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_config = ModelConfig(    \n",
    "    model_config_type = ModelConfigType(\n",
    "        model_type='KERAS',\n",
    "        model_loading_path=os.path.abspath(\n",
    "            f'../data/keras/embedding_lstm_tensorflow_2.h5'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "keras_step = ModelStep(\n",
    "    model_config=keras_config, \n",
    "    parallel_inference_config=ParallelInferenceConfig(workers=1), \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"lstm_1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the server \n",
    "\n",
    "In the `ServingConfig`, specify a port number. \n",
    "\n",
    "The `ServingConfig` has to be passed to `Server` in addition to the steps as a Python list. In this case, there is a single step: `keras_step`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_config = ServingConfig(http_port=1337)\n",
    "\n",
    "server = Server(\n",
    "    serving_config=serving_config, \n",
    "    steps=[keras_step]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server...\n",
      "\n",
      "Server has started successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x193bcb8bb00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure client\n",
    "\n",
    "To configure the client, create a Client object with the port argument. \n",
    "\n",
    "Note that you should create the Client object after the Server has started, so that Client can inherit the Server's attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(port=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict()\n",
    "input_array = np.random.uniform(size = [10])\n",
    "\n",
    "prediction = client.predict({\"input\": np.expand_dims(input_array, axis=0)})\n",
    "\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-3.53171281e-03 -6.55398145e-03 -9.07615386e-03 -1.11385630e-02\n",
      "   -1.27962595e-02 -1.41087854e-02 -1.51341194e-02 -1.59251783e-02\n",
      "   -1.65283810e-02 -1.69831831e-02]\n",
      "  [-8.98816856e-04 -2.16921209e-03 -3.50447348e-03 -4.74313367e-03\n",
      "   -5.81311202e-03 -6.69444632e-03 -7.39536854e-03 -7.93745089e-03\n",
      "   -8.34691338e-03 -8.64967890e-03]\n",
      "  [ 4.57964110e-04  7.27277948e-04  8.35937157e-04  8.26211413e-04\n",
      "    7.38725299e-04  6.06592686e-04  4.54327965e-04  2.98691681e-04\n",
      "    1.50312349e-04  1.51993027e-05]\n",
      "  [-3.72108625e-04 -7.85349868e-04 -1.15279201e-03 -1.44809973e-03\n",
      "   -1.67194800e-03 -1.83526019e-03 -1.95118226e-03 -2.03184737e-03\n",
      "   -2.08712765e-03 -2.12461245e-03]\n",
      "  [-1.70938822e-03 -2.67439662e-03 -3.14217363e-03 -3.29596084e-03\n",
      "   -3.26494290e-03 -3.13617825e-03 -2.96555483e-03 -2.78691365e-03\n",
      "   -2.61911890e-03 -2.47139740e-03]\n",
      "  [ 4.90737101e-03  8.10048729e-03  1.01506319e-02  1.14426687e-02\n",
      "    1.22359265e-02  1.27047608e-02  1.29658673e-02  1.30968681e-02\n",
      "    1.31488722e-02  1.31550385e-02]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 6, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prediction) \n",
    "prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
