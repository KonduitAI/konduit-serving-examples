{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve a Keras model with Konduit Serving \n",
    "\n",
    "This notebook illustrates a simple client-server interaction to perform inference on a Keras LSTM model using the Python SDK for Konduit Serving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konduit import ModelConfig, ParallelInferenceConfig, ModelConfigType, \\\n",
    "ModelStep, ServingConfig\n",
    "\n",
    "from konduit.server import Server\n",
    "from konduit.client import Client\n",
    "import os \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving models in Keras H5 format\n",
    "\n",
    "Models can be saved with the `.save()` method. Refer to the [TensorFlow documentation for Keras](https://www.tensorflow.org/guide/keras/save_and_serialize) for details. \n",
    "\n",
    "Note: Keras model loading functionality in Konduit Serving converts Keras models to Deeplearning4J models. As a result, Keras models containing operations not supported in Deeplearning4J cannot be served in Konduit Serving. See [issue 8348](https://github.com/eclipse/deeplearning4j/issues/8348). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the server\n",
    "\n",
    "Konduit Serving works by defining a series of **steps**. These include operations such as \n",
    "1. Pre- or post-processing steps\n",
    "2. One or more machine learning models\n",
    "3. Transforming the output in a way that can be understood by humans\n",
    "\n",
    "If deploying your model does not require pre- nor post-processing, only one step - a machine learning model - is required. This configuration is defined using a single `ModelStep`. \n",
    "\n",
    "Before running this notebook, run the `build_jar.py` script and copy the JAR (`konduit.jar`) to this folder. Refer to the [Python SDK README](https://github.com/KonduitAI/konduit-serving/blob/master/python/README.md) for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring step \n",
    "\n",
    "Define the Keras configuration as a `ModelConfig` object. \n",
    "- `model_config_type`: This argument requires a `ModelConfigType` object. In the Java program above, we recognised that SimpleCNN is configured as a MultiLayerNetwork, in contrast with the ComputationGraph class, which is used for more complex networks. Specify `model_type` as `KERAS`, and `model_loading_path` to point to the location of Keras weights saved in the HDF5 file format.\n",
    "\n",
    "For the `ModelStep` object, the  following parameters are specified: \n",
    "- `model_config`: pass the ModelConfig object here \n",
    "- `parallel_inference_config`: specify the number of workers to run in parallel. Here, we specify `workers = 1`.\n",
    "- `input_names`:  names for the input nodes  \n",
    "- `output_names`: names for the output nodes\n",
    "\n",
    "Input and output names can be obtained by visualizing the graph in [Netron](https://github.com/lutzroeder/netron). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_config = ModelConfig(    \n",
    "    model_config_type = ModelConfigType(\n",
    "        model_type='KERAS',\n",
    "        model_loading_path=os.path.abspath(\n",
    "            f'../data/keras/embedding_lstm_tensorflow_2.h5'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "keras_step = ModelStep(\n",
    "    model_config=keras_config, \n",
    "    parallel_inference_config=ParallelInferenceConfig(workers=1), \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"lstm_1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the server \n",
    "\n",
    "In the `ServingConfig`, specify a port number. \n",
    "\n",
    "The `ServingConfig` has to be passed to `Server` in addition to the steps as a Python list. In this case, there is a single step: `keras_step`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_config = ServingConfig(http_port=1337)\n",
    "\n",
    "server = Server(\n",
    "    serving_config=serving_config, \n",
    "    steps=[keras_step]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server..\n",
      "\n",
      "Server has started successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x11b2e11bb48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure client\n",
    "\n",
    "To configure the client, create a Client object with the port argument. \n",
    "\n",
    "Note that you should create the Client object after the Server has started, so that Client can inherit the Server's attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(port=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict()\n",
    "input_array = np.random.uniform(size = [10])\n",
    "\n",
    "prediction = client.predict({\"input\": input_array})\n",
    "\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.49911702 0.4983615  0.49773094 0.49721536 0.496801   0.49647287\n",
      "   0.49621654 0.49601877 0.495868   0.4957543 ]\n",
      "  [0.49977526 0.49945772 0.49912393 0.4988142  0.49854672 0.49832645\n",
      "   0.49815112 0.49801567 0.49791327 0.49783763]\n",
      "  [0.5001145  0.5001818  0.500209   0.50020653 0.50018466 0.50015163\n",
      "   0.50011355 0.5000747  0.5000376  0.5000038 ]\n",
      "  [0.499907   0.49980363 0.4997118  0.499638   0.49958205 0.49954122\n",
      "   0.4995122  0.49949202 0.49947822 0.4994689 ]\n",
      "  [0.49957263 0.4993314  0.49921444 0.49917603 0.4991838  0.499216\n",
      "   0.4992586  0.49930325 0.4993452  0.4993822 ]\n",
      "  [0.50122684 0.5020251  0.5025376  0.50286067 0.50305897 0.50317615\n",
      "   0.5032414  0.50327414 0.50328714 0.50328875]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 6, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prediction) \n",
    "prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
